{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TDTRqrlemlLB"
   },
   "source": [
    "#### Creating README.txt\n",
    "# OBSERVATION: \n",
    "If you have doubts about the code, contact rdr-contacte@csuc.cat\n",
    "\n",
    "## SCRIPT OBJECTIVE\n",
    "The main objective of this script is to automatically create the README file for a dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86,
     "referenced_widgets": [
      "549357a5ba174f0baaa44897b60783f3",
      "acb2ab1485364bb9bbc12e0ff226b28c",
      "2fa080e4e81e44479d2e76ca5327f6b7"
     ]
    },
    "id": "1Wphbk1gl7Fq",
    "outputId": "2d84f207-2f4e-4fd7-b069-61a1c4380b0e"
   },
   "outputs": [],
   "source": [
    "# @title Install or Update Libraries (Click the &#x25B6; button to execute)\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, clear_output\n",
    "\n",
    "# Function to install required packages\n",
    "def install_packages(b):\n",
    "    \"\"\"\n",
    "    Function to install or update necessary Python packages.\n",
    "\n",
    "    Parameters:\n",
    "    - b: Button object. Click event handler.\n",
    "    \"\"\"\n",
    "    clear_output(wait=True)\n",
    "    !pip install --upgrade pip -q  # Upgrade pip silently\n",
    "    !pip install pyDataverse -q  # Install or upgrade pyDataverse silently\n",
    "    !pip install html2text -q  # Install or upgrade html2text silently\n",
    "    print(\"Libraries have been downloaded or updated.\")\n",
    "\n",
    "# Displaying installation message\n",
    "display(HTML(\"<p style='font-size:14px;'><b>Click the following button to install the libraries.</b></p>\"))\n",
    "\n",
    "# Creating installation button\n",
    "install_button = widgets.Button(description='Install Libraries')\n",
    "install_button.on_click(install_packages)\n",
    "\n",
    "# Displaying the installation button\n",
    "display(install_button)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "93520107254c45a9b49e4c1fc05da9ec",
      "bbe678f714d148ffa454f85278baaa1e",
      "6e228675cbb042cbb2d3604308c8d5c6"
     ]
    },
    "id": "0G7zzKeZl-Hv",
    "outputId": "d08baa6d-bf3f-4019-ac43-7d2d88d6320f"
   },
   "outputs": [],
   "source": [
    "# @title Enter DOI (doi:10.34810/dataXXX), API token and the repository URL. Click the &#x25B6; button to execute cell\n",
    "\n",
    "import os\n",
    "from IPython.display import display, FileLink\n",
    "try:\n",
    "    from google.colab import files\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "from pyDataverse.api import NativeApi, DataAccessApi\n",
    "from pyDataverse.models import Dataverse\n",
    "\n",
    "# Provide input values\n",
    "doi = \"\"  # @param {type:\"string\"}  \n",
    "token = \"\"  # @param {type:\"string\"}  \n",
    "base_url = \"https://dataverse.csuc.cat/\" # @param {type:\"string\"}\n",
    "\n",
    "def extract_value(data_dict):\n",
    "    \"\"\"\n",
    "    Function to extract all keys and values from a JSON metadata dictionary.\n",
    "\n",
    "    Parameters:\n",
    "    - data_dict: dict. JSON metadata dictionary.\n",
    "\n",
    "    Returns:\n",
    "    - type_names: list. List of type names extracted from the metadata.\n",
    "    - values: list. List of values extracted from the metadata.\n",
    "    \"\"\"\n",
    "    if isinstance(data_dict, dict):\n",
    "        type_names = []\n",
    "        values = []\n",
    "        for key, value in data_dict.items():\n",
    "            if key == 'typeName' and 'value' in data_dict:\n",
    "                if isinstance(data_dict['value'], list):\n",
    "                    for v in data_dict['value']:\n",
    "                        type_names.append(data_dict['typeName'])\n",
    "                        values.append(v)\n",
    "                else:\n",
    "                    type_names.append(data_dict['typeName'])\n",
    "                    values.append(data_dict['value'])\n",
    "            elif isinstance(value, dict) and 'typeName' in value and 'value' in value:\n",
    "                type_names.append(value['typeName'])\n",
    "                values.append(value['value'])\n",
    "            elif isinstance(value, str) and key == 'typeName':\n",
    "                type_names.append(value)\n",
    "                values.append(value)\n",
    "            else:\n",
    "                extracted_type_names, extracted_values = extract_value(value)\n",
    "                type_names += extracted_type_names\n",
    "                values += extracted_values\n",
    "        return type_names, values\n",
    "    elif isinstance(data_dict, list):\n",
    "        type_names = []\n",
    "        values = []\n",
    "        for item in data_dict:\n",
    "            extracted_type_names, extracted_values = extract_value(item)\n",
    "            type_names += extracted_type_names\n",
    "            values += extracted_values\n",
    "        return type_names, values\n",
    "    else:\n",
    "        return [], []\n",
    "def exportmetadata(base_url, token, doi, citation_keys, citation_values, geo_keys, geo_values, social_keys, social_values, astronomy_keys, astronomy_values, biomedical_keys, biomedical_values, journal_keys, journal_values):\n",
    "    \"\"\"\n",
    "    Function to export metadata from a dataset identified by its DOI.\n",
    "\n",
    "    Parameters:\n",
    "    - base_url: str. Base URL of the Dataverse instance.\n",
    "    - token: str. API token for authentication.\n",
    "    - doi: str. DOI of the dataset.\n",
    "    - citation_keys: list. List to store citation metadata keys.\n",
    "    - citation_values: list. List to store citation metadata values.\n",
    "    - geo_keys: list. List to store geospatial metadata keys.\n",
    "    - geo_values: list. List to store geospatial metadata values.\n",
    "    - social_keys: list. List to store social science metadata keys.\n",
    "    - social_values: list. List to store social science metadata values.\n",
    "    - astronomy_keys: list. List to store astronomy metadata keys.\n",
    "    - astronomy_values: list. List to store astronomy metadata values.\n",
    "    - biomedical_keys: list. List to store biomedical metadata keys.\n",
    "    - biomedical_values: list. List to store biomedical metadata values.\n",
    "    - journal_keys: list. List to store journal metadata keys.\n",
    "    - journal_values: list. List to store journal metadata values.\n",
    "\n",
    "    Returns:\n",
    "    - None. Updates the provided lists with extracted metadata.\n",
    "    \"\"\"\n",
    "    from pyDataverse.api import NativeApi, DataAccessApi\n",
    "    from pyDataverse.models import Dataverse\n",
    "    import html2text\n",
    "    import os\n",
    "\n",
    "    # Instantiate API objects for accessing Dataverse\n",
    "    api = NativeApi(base_url, token)\n",
    "    data_api = DataAccessApi(base_url, token)\n",
    "\n",
    "    try:\n",
    "        # Retrieve dataset metadata\n",
    "        dataset = api.get_dataset(doi)\n",
    "\n",
    "        # Extract citation metadata if available\n",
    "        if 'citation' in dataset.json()['data']['latestVersion']['metadataBlocks']:\n",
    "            metadata_citation = dataset.json()['data']['latestVersion']['metadataBlocks']['citation']['fields']\n",
    "            citation = extract_value(metadata_citation)\n",
    "            citation_keys.extend(citation[0])\n",
    "            citation_values.extend(citation[1])\n",
    "            for item in metadata_citation:\n",
    "                if isinstance(item['value'], str):\n",
    "                    index_canvi = citation_keys.index(item['typeName'])\n",
    "                    citation_values[index_canvi] = item['value']\n",
    "\n",
    "        # Extract geospatial metadata if available\n",
    "        if 'geospatial' in dataset.json()['data']['latestVersion']['metadataBlocks']:\n",
    "            metadata_geospatial = dataset.json()['data']['latestVersion']['metadataBlocks']['geospatial']['fields']\n",
    "            geospatial = extract_value(metadata_geospatial)\n",
    "            geo_keys.extend(geospatial[0])\n",
    "            geo_values.extend(geospatial[1])\n",
    "            for item in metadata_geospatial:\n",
    "                if isinstance(item['value'], str):\n",
    "                    index_canvi = geo_keys.index(item['typeName'])\n",
    "                    geo_values[index_canvi] = item['value']\n",
    "\n",
    "        # Extract social science metadata if available\n",
    "        if 'socialscience' in dataset.json()['data']['latestVersion']['metadataBlocks']:\n",
    "            metadata_socialscience = dataset.json()['data']['latestVersion']['metadataBlocks']['socialscience']['fields']\n",
    "            socialscience = extract_value(metadata_socialscience)\n",
    "            social_keys.extend(socialscience[0])\n",
    "            social_values.extend(socialscience[1])\n",
    "            for item in metadata_socialscience:\n",
    "                if isinstance(item['value'], str):\n",
    "                    index_canvi = social_keys.index(item['typeName'])\n",
    "                    social_values[index_canvi] = item['value']\n",
    "\n",
    "        # Extract astronomy metadata if available\n",
    "        if 'astrophysics' in dataset.json()['data']['latestVersion']['metadataBlocks']:\n",
    "            metadata_astronomy = dataset.json()['data']['latestVersion']['metadataBlocks']['astrophysics']['fields']\n",
    "            astronomy = extract_value(metadata_astronomy)\n",
    "            astronomy_keys.extend(astronomy[0])\n",
    "            astronomy_values.extend(astronomy[1])\n",
    "            for item in metadata_astronomy:\n",
    "                if isinstance(item['value'], str):\n",
    "                    index_canvi = astronomy_keys.index(item['typeName'])\n",
    "                    astronomy_values[index_canvi] = item['value']\n",
    "\n",
    "        # Extract biomedical metadata if available\n",
    "        if 'biomedical' in dataset.json()['data']['latestVersion']['metadataBlocks']:\n",
    "            metadata_biomedical = dataset.json()['data']['latestVersion']['metadataBlocks']['biomedical']['fields']\n",
    "            biomedical = extract_value(metadata_biomedical)\n",
    "            biomedical_keys.extend(biomedical[0])\n",
    "            biomedical_values.extend(biomedical[1])\n",
    "            for item in metadata_biomedical:\n",
    "                if isinstance(item['value'], str):\n",
    "                    index_canvi = biomedical_keys.index(item['typeName'])\n",
    "                    biomedical_values[index_canvi] = item['value']\n",
    "\n",
    "        # Extract journal metadata if available\n",
    "        if 'journal' in dataset.json()['data']['latestVersion']['metadataBlocks']:\n",
    "            metadata_journal = dataset.json()['data']['latestVersion']['metadataBlocks']['journal']['fields']\n",
    "            journal = extract_value(metadata_journal)\n",
    "            journal_keys.extend(journal[0])\n",
    "            journal_values.extend(journal[1])\n",
    "            for item in metadata_journal:\n",
    "                if isinstance(item['value'], str):\n",
    "                    index_canvi = journal_keys.index(item['typeName'])\n",
    "                    journal_values[index_canvi] = item['value']\n",
    "\n",
    "    except (KeyError, InvalidSchema) as e:\n",
    "        # Catch specific exceptions and print the error message\n",
    "        print(f\"Error occurred: {e}\")\n",
    "        print('There was an error reading metadata for the dataset: ' + doi)\n",
    "\n",
    "def filemetadata(base_url, token, doi, filemetadata_keys, filemetadata_values):\n",
    "    \"\"\"\n",
    "    Function to extract metadata for files associated with a dataset identified by its DOI.\n",
    "\n",
    "    Parameters:\n",
    "    - base_url: str. Base URL of the Dataverse instance.\n",
    "    - token: str. API token for authentication.\n",
    "    - doi: str. DOI of the dataset.\n",
    "    - filemetadata_keys: list. List to store file metadata keys.\n",
    "    - filemetadata_values: list. List to store file metadata values.\n",
    "\n",
    "    Returns:\n",
    "    - None. Updates the provided lists with extracted file metadata.\n",
    "    \"\"\"\n",
    "    from pyDataverse.api import NativeApi, DataAccessApi\n",
    "    from pyDataverse.models import Dataverse\n",
    "\n",
    "    # Instantiate API objects for accessing Dataverse\n",
    "    api = NativeApi(base_url, token)\n",
    "    data_api = DataAccessApi(base_url, token)\n",
    "\n",
    "    try:\n",
    "        # Retrieve dataset metadata\n",
    "        dataset = api.get_dataset(doi)\n",
    "\n",
    "        # Iterate through files and extract metadata\n",
    "        for i in range(len(dataset.json()['data']['latestVersion']['files'])):\n",
    "            filemetadata_resp = dataset.json()['data']['latestVersion']['files'][i]['dataFile']\n",
    "            filemetadata_keys_aux = list(filemetadata_resp.keys())\n",
    "            filemetadata_values_aux = list(filemetadata_resp.values())\n",
    "            filemetadata_keys.append(filemetadata_keys_aux)\n",
    "            filemetadata_values.append(filemetadata_values_aux)\n",
    "    except KeyError:\n",
    "        print('There was an error reading metadata for the files of the dataset: ' + doi)\n",
    "\n",
    "def list_duplicates_of(seq, item):\n",
    "    \"\"\"\n",
    "    Function to list indexes of duplicates of an item in a sequence.\n",
    "\n",
    "    Parameters:\n",
    "    - seq: list. Sequence to search for duplicates.\n",
    "    - item: any. Item to search for duplicates.\n",
    "\n",
    "    Returns:\n",
    "    - locs: list. List of indexes where the item occurs more than once in the sequence.\n",
    "    \"\"\"\n",
    "    start_at = -1\n",
    "    locs = []\n",
    "    while True:\n",
    "        try:\n",
    "            loc = seq.index(item, start_at + 1)\n",
    "        except ValueError:\n",
    "            break\n",
    "        else:\n",
    "            locs.append(loc)\n",
    "            start_at = loc\n",
    "    return locs\n",
    "\n",
    "def find_keys(keys, specified_keys, values):\n",
    "    \"\"\"\n",
    "    Function to find specified keys and their corresponding values in a list of keys and values.\n",
    "\n",
    "    Parameters:\n",
    "    - keys: list. List of keys.\n",
    "    - specified_keys: list. List of specified keys to find.\n",
    "    - values: list. List of values corresponding to the keys.\n",
    "\n",
    "    Returns:\n",
    "    - extracted_values: list. List of dictionaries containing extracted key-value pairs.\n",
    "    \"\"\"\n",
    "    # Dictionary to store extracted values\n",
    "    extracted_values = []\n",
    "    # Dictionary to store current entry\n",
    "    current_entry = {}\n",
    "    # Set to keep track of found keys\n",
    "    found_keys = set()\n",
    "    # Iterate through keys and values\n",
    "    for key, value in zip(keys, values):\n",
    "        # Check if current key is in specified keys\n",
    "        if key in specified_keys:\n",
    "            current_entry[key] = value\n",
    "            found_keys.add(key)\n",
    "            # If all specified keys are found, add entry to extracted_values\n",
    "            if len(found_keys) == len(specified_keys):\n",
    "                extracted_values.append(current_entry)\n",
    "                current_entry = {}  # Reset current entry\n",
    "                found_keys.clear()  # Clear found keys set for next entry\n",
    "    return extracted_values\n",
    "\n",
    "def format_key(key):\n",
    "    \"\"\"\n",
    "    Function to format a key by splitting camel case and capitalizing the first letter of each word.\n",
    "\n",
    "    Parameters:\n",
    "    - key: str. Key to be formatted.\n",
    "\n",
    "    Returns:\n",
    "    - formatted_key: str. Formatted key.\n",
    "    \"\"\"\n",
    "    words = []\n",
    "    current_word = ''\n",
    "    for char in key:\n",
    "        if char.isupper() and current_word:\n",
    "            words.append(current_word)\n",
    "            current_word = char\n",
    "        else:\n",
    "            current_word += char\n",
    "    if current_word:\n",
    "        words.append(current_word)\n",
    "\n",
    "    formatted_key = ' '.join(words)\n",
    "    return formatted_key.capitalize()\n",
    "\n",
    "def createreadme(base_url, token, doi, citation_keys, citation_values, geo_keys, geo_values, social_keys, social_values, astronomy_keys, astronomy_values, biomedical_keys, biomedical_values, journal_keys, journal_values, filemetadata_keys, filemetadata_values):\n",
    "    \"\"\"\n",
    "    Function to create a readme file for a dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - base_url: str. Base URL of the Dataverse instance.\n",
    "    - token: str. API token for authentication.\n",
    "    - doi: str. DOI of the dataset.\n",
    "    - citation_keys: list. List of citation metadata keys.\n",
    "    - citation_values: list. List of citation metadata values.\n",
    "    - geo_keys: list. List of geospatial metadata keys.\n",
    "    - geo_values: list. List of geospatial metadata values.\n",
    "    - social_keys: list. List of social science metadata keys.\n",
    "    - social_values: list. List of social science metadata values.\n",
    "    - astronomy_keys: list. List of astronomy metadata keys.\n",
    "    - astronomy_values: list. List of astronomy metadata values.\n",
    "    - biomedical_keys: list. List of biomedical metadata keys.\n",
    "    - biomedical_values: list. List of biomedical metadata values.\n",
    "    - journal_keys: list. List of journal metadata keys.\n",
    "    - journal_values: list. List of journal metadata values.\n",
    "    - filemetadata_keys: list. List of file metadata keys.\n",
    "    - filemetadata_values: list. List of file metadata values.\n",
    "    \"\"\"\n",
    "\n",
    "    # Import necessary libraries\n",
    "    from pyDataverse.api import NativeApi, DataAccessApi\n",
    "    from pyDataverse.models import Dataverse\n",
    "    import os\n",
    "    import html2text\n",
    "\n",
    "    # Instantiate API objects for accessing Dataverse\n",
    "    api = NativeApi(base_url, token)\n",
    "    data_api = DataAccessApi(base_url, token)\n",
    "\n",
    "    # Retrieve dataset metadata\n",
    "    dataset = api.get_dataset(doi)\n",
    "\n",
    "    # Extract path from DOI\n",
    "    path = doi.replace(\"doi:10.34810/\", \"\")\n",
    "\n",
    "    try:\n",
    "        # Create directory if it does not exist\n",
    "        os.mkdir(path)\n",
    "    except OSError:\n",
    "        print(\"Directory \" + path + ' already exists. The Readme will be saved in this directory.')\n",
    "\n",
    "    with open(path + '/' + 'Readme.txt', 'w', encoding='utf-8') as f:\n",
    "        f.write('GENERAL INFORMATION\\n------------------\\n')\n",
    "        cont = 0\n",
    "\n",
    "        # Write metadata to Readme file\n",
    "        if 'PreviousDatasetPersistentID' in citation_keys:\n",
    "            cont += 1\n",
    "            f.write(str(cont) + '.  Previous Dataset Persistent ID:\\n')\n",
    "            auxiliar = []\n",
    "            auxiliar.append(list_duplicates_of(citation_keys, 'PreviousDatasetPersistentID'))\n",
    "            for i in auxiliar[0]:\n",
    "                f.write('\\t')\n",
    "                f.write(citation_values[i])\n",
    "                if i != auxiliar[0][-1]:\n",
    "                    f.write('\\n ')\n",
    "            f.write('\\n\\n')\n",
    "\n",
    "        if 'title' in citation_keys:\n",
    "            cont += 1\n",
    "            f.write(str(cont) + '.  Dataset title:\\n')\n",
    "            f.write('\\t' + citation_values[citation_keys.index('title')] + '\\n\\n')  # Write the title\n",
    "\n",
    "        if 'authorName' in citation_keys or 'authorAffiliation' in citation_keys or 'authorIdentifierScheme' in citation_keys or 'authorIdentifier' in citation_keys:\n",
    "            cont+=1\n",
    "            f.write(str(cont)+'.  Authorship:\\n')\n",
    "            keys=['authorName','authorAffiliation','authorIdentifierScheme','authorIdentifier' ]\n",
    "            specified_keys = [element for element in keys if element in citation_keys]\n",
    "            extracted_values = find_keys(citation_keys, specified_keys, citation_values)\n",
    "            for entry in extracted_values:\n",
    "                for key, value in entry.items():\n",
    "                    formatted_key = format_key(key)\n",
    "                    f.write('\\t'+f'{formatted_key}: {value}\\n')\n",
    "                f.write('\\n')\n",
    "        if 'datasetContactName' in citation_keys or 'datasetContactAffiliation' in citation_keys or 'datasetContactEmail' in citation_keys:\n",
    "            cont+=1\n",
    "            f.write(str(cont)+'.  Dataset contact:\\n')\n",
    "            keys=['datasetContactName','datasetContactAffiliation','datasetContactEmail' ]\n",
    "            specified_keys = [element for element in keys if element in citation_keys]\n",
    "            extracted_values = find_keys(citation_keys, specified_keys, citation_values)\n",
    "            for entry in extracted_values:\n",
    "                for key, value in entry.items():\n",
    "                    formatted_key = format_key(key)\n",
    "                    f.write('\\t'+f'{formatted_key}: {value}\\n')\n",
    "                f.write('\\n')\n",
    "        f.write('DESCRIPTION\\n----------\\n')\n",
    "        cont=0\n",
    "        if 'language' in citation_keys:\n",
    "            cont+=1\n",
    "            f.write(str(cont)+'.  Dataset language:\\n')\n",
    "            auxiliar=[]\n",
    "            auxiliar.append(list_duplicates_of(citation_keys, 'language'))\n",
    "            for i in auxiliar[0]:\n",
    "                f.write('\\t')\n",
    "                f.write(citation_values[i])\n",
    "                if i != auxiliar[0][-1]:\n",
    "                    f.write('\\n ')\n",
    "            f.write('\\n\\n')\n",
    "        if 'dsDescriptionValue' in citation_keys:\n",
    "            cont+=1\n",
    "            f.write(str(cont)+'.  Abstract:\\n')\n",
    "            auxiliar=[]\n",
    "            auxiliar.append(list_duplicates_of(citation_keys, 'dsDescriptionValue'))\n",
    "            for i in auxiliar[0]:\n",
    "                f.write('\\t')\n",
    "                f.write(citation_values[i])\n",
    "                if i != auxiliar[0][-1]:\n",
    "                    f.write('\\n ')\n",
    "            f.write('\\n\\n')\n",
    "        if 'subject' in citation_keys:\n",
    "            cont+=1\n",
    "            f.write(str(cont)+'.  Subject:\\n')\n",
    "            auxiliar=[]\n",
    "            auxiliar.append(list_duplicates_of(citation_keys, 'subject'))\n",
    "            for i in auxiliar[0]:\n",
    "                f.write('\\t')\n",
    "                f.write(citation_values[i])\n",
    "                if i != auxiliar[0][-1]:\n",
    "                    f.write('\\n ')\n",
    "            f.write('\\n\\n')\n",
    "        if 'keywordValue' in citation_keys or 'keywordVocabulary' in citation_keys or 'keywordVocabularyURI' in citation_keys:\n",
    "            cont+=1\n",
    "            f.write(str(cont)+'.  Keyword:\\n')\n",
    "            keys=['keywordValue','keywordVocabulary','keywordVocabularyURI' ]\n",
    "            specified_keys = [element for element in keys if element in citation_keys]\n",
    "            extracted_values = find_keys(citation_keys, specified_keys, citation_values)\n",
    "            for entry in extracted_values:\n",
    "                for key, value in entry.items():\n",
    "                    formatted_key = format_key(key)\n",
    "                    f.write('\\t'+f'{formatted_key}: {value}\\n')\n",
    "                f.write('\\n')\n",
    "        if 'topicClassValue' in citation_keys or 'topicClassVocab' in citation_keys or 'topicClassVocabURI' in citation_keys:\n",
    "            cont+=1\n",
    "            f.write(str(cont)+'.  Topic classification:\\n')\n",
    "            keys=['topicClassValue','topicClassVocab','topicClassVocabURI' ]\n",
    "            specified_keys = [element for element in keys if element in citation_keys]\n",
    "            extracted_values = find_keys(citation_keys, specified_keys, citation_values)\n",
    "            for entry in extracted_values:\n",
    "                for key, value in entry.items():\n",
    "                    formatted_key = format_key(key)\n",
    "                    f.write('\\t'+f'{formatted_key}: {value}\\n')\n",
    "                f.write('\\n')\n",
    "        if 'notesText' in citation_keys:\n",
    "            cont+=1\n",
    "            f.write(str(cont)+'.  Notes:\\n')\n",
    "            f.write('\\t'+citation_values[citation_keys.index('notesText')]+'\\n\\n')\n",
    "        if 'producerName' in citation_keys or 'producerAffiliation' in citation_keys or 'producerAbbreviation' in citation_keys or 'producerURL' in citation_keys:\n",
    "            cont+=1\n",
    "            f.write(str(cont)+'.  Producer:\\n')\n",
    "            keys=['producerName','producerAffiliation','producerAbbreviation','producerURL' ]\n",
    "            specified_keys = [element for element in keys if element in citation_keys]\n",
    "            extracted_values = find_keys(citation_keys, specified_keys, citation_values)\n",
    "            for entry in extracted_values:\n",
    "                for key, value in entry.items():\n",
    "                    formatted_key = format_key(key)\n",
    "                    f.write('\\t'+f'{formatted_key}: {value}\\n')\n",
    "                f.write('\\n')\n",
    "        if 'productionDate' in citation_keys:\n",
    "            cont+=1\n",
    "            f.write(str(cont)+'.  Production date:\\n')\n",
    "            f.write('\\t'+citation_values[citation_keys.index('productionDate')]+'\\n\\n')\n",
    "        if 'Production place' in citation_keys:\n",
    "            cont+=1\n",
    "            f.write(str(cont)+'.  Production place:\\n')\n",
    "            f.write('\\t'+citation_values[citation_keys.index('productionPlace')]+'\\n\\n')\n",
    "        if 'contributorType' in citation_keys or 'contributorName' in citation_keys:\n",
    "            cont+=1\n",
    "            f.write(str(cont)+'.  Contributor:\\n')\n",
    "            keys=['contributorType','contributorName']\n",
    "            specified_keys = [element for element in keys if element in citation_keys]\n",
    "            extracted_values = find_keys(citation_keys, specified_keys, citation_values)\n",
    "            for entry in extracted_values:\n",
    "                for key, value in entry.items():\n",
    "                    formatted_key = format_key(key)\n",
    "                    f.write('\\t'+f'{formatted_key}: {value}\\n')\n",
    "                f.write('\\n')\n",
    "        if 'grantNumberAgency' in citation_keys or 'grantNumberValue' in citation_keys:\n",
    "            cont+=1\n",
    "            f.write(str(cont)+'. Grant information:\\n')\n",
    "            keys=['grantNumberAgency','grantNumberValue']\n",
    "            specified_keys = [element for element in keys if element in citation_keys]\n",
    "            extracted_values = find_keys(citation_keys, specified_keys, citation_values)\n",
    "            for entry in extracted_values:\n",
    "                for key, value in entry.items():\n",
    "                    formatted_key = format_key(key)\n",
    "                    f.write('\\t'+f'{formatted_key}: {value}\\n')\n",
    "                f.write('\\n')\n",
    "        if 'distributorName' in citation_keys or 'distributorAffiliation' in citation_keys or 'distributorAbbreviation' in citation_keys or 'distributorURL' in citation_keys:\n",
    "            cont+=1\n",
    "            f.write(str(cont)+'.  Distributor:\\n')\n",
    "            keys=['distributorName','distributorAffiliation','distributorAbbreviation','distributorURL' ]\n",
    "            specified_keys = [element for element in keys if element in citation_keys]\n",
    "            extracted_values = find_keys(citation_keys, specified_keys, citation_values)\n",
    "            for entry in extracted_values:\n",
    "                for key, value in entry.items():\n",
    "                    formatted_key = format_key(key)\n",
    "                    f.write('\\t'+f'{formatted_key}: {value}\\n')\n",
    "                f.write('\\n')\n",
    "        if 'distributionDate' in citation_keys:\n",
    "            cont+=1\n",
    "            f.write(str(cont)+'.  Distribution date:\\n')\n",
    "            f.write('\\t'+citation_values[citation_keys.index('distributionDate')]+'\\n\\n')\n",
    "        if 'depositor' in citation_keys:\n",
    "            cont+=1\n",
    "            f.write(str(cont)+'.  Depositor:\\n')\n",
    "            f.write('\\t'+citation_values[citation_keys.index('depositor')]+'\\n\\n')\n",
    "        if 'dateOfDeposit' in citation_keys:\n",
    "            cont+=1\n",
    "            f.write(str(cont)+'.  Date of deposit:\\n')\n",
    "            f.write('\\t'+citation_values[citation_keys.index('dateOfDeposit')]+'\\n\\n')\n",
    "        if 'timePeriodCoveredStart' in citation_keys or 'timePeriodCoveredEnd' in citation_keys:\n",
    "            cont+=1\n",
    "            f.write(str(cont)+'.  Time period covered (single date or date range):\\n')\n",
    "            keys=['timePeriodCoveredStart','timePeriodCoveredEnd']\n",
    "            specified_keys = [element for element in keys if element in citation_keys]\n",
    "            extracted_values = find_keys(citation_keys, specified_keys, citation_values)\n",
    "            for entry in extracted_values:\n",
    "                for key, value in entry.items():\n",
    "                    formatted_key = format_key(key)\n",
    "                    f.write('\\t'+f'{formatted_key}: {value}\\n')\n",
    "                f.write('\\n')\n",
    "        if 'dateOfCollectionStart' in citation_keys or 'dateOfCollectionEnd' in citation_keys:\n",
    "            cont+=1\n",
    "            f.write(str(cont)+'.  Date of data collection (single date or date range):\\n')\n",
    "            keys=['dateOfCollectionStart','dateOfCollectionEnd']\n",
    "            specified_keys = [element for element in keys if element in citation_keys]\n",
    "            extracted_values = find_keys(citation_keys, specified_keys, citation_values)\n",
    "            for entry in extracted_values:\n",
    "                for key, value in entry.items():\n",
    "                    formatted_key = format_key(key)\n",
    "                    f.write('\\t'+f'{formatted_key}: {value}\\n')\n",
    "                f.write('\\n')\n",
    "        if 'publicationDate' in dataset.json()['data']:\n",
    "            cont+=1\n",
    "            f.write(str(cont)+'.  Publication Date:\\n')\n",
    "            f.write('\\t'+dataset.json()['data']['publicationDate']+'\\n\\n')\n",
    "        if 'dateOfCollectionStart' not in citation_keys and 'publicationDate' not in dataset.json()['data']:\n",
    "            f.write('\\n')\n",
    "        if 'kindOfData' in citation_keys:\n",
    "            cont+=1\n",
    "            f.write(str(cont)+'.  Kind of Data:\\n')\n",
    "            auxiliar=[]\n",
    "            auxiliar.append(list_duplicates_of(citation_keys, 'kindOfData'))\n",
    "            for i in auxiliar[0]:\n",
    "                f.write('\\t')\n",
    "                f.write(citation_values[i])\n",
    "                if i != auxiliar[0][-1]:\n",
    "                    f.write('\\n ')\n",
    "            f.write('\\n\\n')\n",
    "        if 'seriesName' in citation_keys or 'seriesInformation' in citation_keys:\n",
    "            cont+=1\n",
    "            f.write(str(cont)+'. Series:\\n')\n",
    "            index=[]\n",
    "            keys=['seriesName','seriesInformation']\n",
    "            specified_keys=[]\n",
    "            specified_keys = [element for element in keys if element in citation_keys]\n",
    "            extracted_values = find_keys(citation_keys, specified_keys, citation_values)\n",
    "            for entry in extracted_values:\n",
    "                for key, value in entry.items():\n",
    "                    formatted_key = format_key(key)\n",
    "                    f.write('\\t'+f'{formatted_key}: {value}\\n')\n",
    "                f.write('\\n')\n",
    "        if 'softwareName' in citation_keys or 'softwareVersion' in citation_keys:\n",
    "            cont+=1\n",
    "            f.write(str(cont)+'. Software:\\n')\n",
    "            keys=['softwareName','softwareVersion']\n",
    "            specified_keys = [element for element in keys if element in citation_keys]\n",
    "            extracted_values = find_keys(citation_keys, specified_keys, citation_values)\n",
    "            for entry in extracted_values:\n",
    "                for key, value in entry.items():\n",
    "                    formatted_key = format_key(key)\n",
    "                    f.write('\\t'+f'{formatted_key}: {value}\\n')\n",
    "                f.write('\\n')\n",
    "        if 'relatedMaterial' in citation_keys:\n",
    "            cont+=1\n",
    "            f.write(str(cont)+'.  Related material:\\n')\n",
    "            auxiliar=[]\n",
    "            auxiliar.append(list_duplicates_of(citation_keys, 'relatedMaterial'))\n",
    "            for i in auxiliar[0]:\n",
    "                f.write('\\t')\n",
    "                f.write(citation_values[i])\n",
    "                if i != auxiliar[0][-1]:\n",
    "                    f.write('\\n ')\n",
    "            f.write('\\n\\n')\n",
    "        if 'relatedDatasets' in citation_keys:\n",
    "            cont+=1\n",
    "            f.write(str(cont)+'.  Related datasets:\\n')\n",
    "            auxiliar=[]\n",
    "            auxiliar.append(list_duplicates_of(citation_keys, 'relatedDatasets'))\n",
    "            for i in auxiliar[0]:\n",
    "                f.write('\\t')\n",
    "                f.write(citation_values[i])\n",
    "                if i != auxiliar[0][-1]:\n",
    "                    f.write('\\n ')\n",
    "            f.write('\\n\\n')\n",
    "        if 'otherReferences' in citation_keys:\n",
    "            cont+=1\n",
    "            f.write(str(cont)+'.  Other references:\\n')\n",
    "            auxiliar=[]\n",
    "            auxiliar.append(list_duplicates_of(citation_keys, 'otherReferences'))\n",
    "            for i in auxiliar[0]:\n",
    "                f.write('\\t')\n",
    "                f.write(citation_values[i])\n",
    "                if i != auxiliar[0][-1]:\n",
    "                    f.write('\\n ')\n",
    "            f.write('\\n\\n')\n",
    "        if 'dataSources' in citation_keys:\n",
    "            cont+=1\n",
    "            f.write(str(cont)+'.  Data sources:\\n')\n",
    "            auxiliar=[]\n",
    "            auxiliar.append(list_duplicates_of(citation_keys, 'dataSources'))\n",
    "            for i in auxiliar[0]:\n",
    "                f.write('\\t')\n",
    "                f.write(citation_values[i])\n",
    "                if i != auxiliar[0][-1]:\n",
    "                    f.write('\\n ')\n",
    "            f.write('\\n\\n')\n",
    "        if 'originOfSources' in citation_keys:\n",
    "            cont+=1\n",
    "            f.write(str(cont)+'.  Origin of sources:\\n')\n",
    "            auxiliar=[]\n",
    "            auxiliar.append(list_duplicates_of(citation_keys, 'originOfSources'))\n",
    "            for i in auxiliar[0]:\n",
    "                f.write('\\t')\n",
    "                f.write(citation_values[i])\n",
    "                if i != auxiliar[0][-1]:\n",
    "                    f.write('\\n ')\n",
    "            f.write('\\n\\n')\n",
    "        if 'characteristicOfSources' in citation_keys:\n",
    "            cont+=1\n",
    "            f.write(str(cont)+'.  Characteristic of sources:\\n')\n",
    "            auxiliar=[]\n",
    "            auxiliar.append(list_duplicates_of(citation_keys, 'characteristicOfSources'))\n",
    "            for i in auxiliar[0]:\n",
    "                f.write('\\t')\n",
    "                f.write(citation_values[i])\n",
    "                if i != auxiliar[0][-1]:\n",
    "                    f.write('\\n ')\n",
    "            f.write('\\n\\n')\n",
    "        if 'accessToSources' in citation_keys:\n",
    "            cont+=1\n",
    "            f.write(str(cont)+'.  Access to sources:\\n')\n",
    "            auxiliar=[]\n",
    "            auxiliar.append(list_duplicates_of(citation_keys, 'accessToSources'))\n",
    "            for i in auxiliar[0]:\n",
    "                f.write('\\t')\n",
    "                f.write(citation_values[i])\n",
    "                if i != auxiliar[0][-1]:\n",
    "                    f.write('\\n ')\n",
    "            f.write('\\n\\n')\n",
    "        f.write('ACCESS INFORMATION\\n------------------------\\n')\n",
    "        cont=0\n",
    "        if 'license' in dataset.json()['data']['latestVersion']:\n",
    "            cont+=1\n",
    "            f.write(str(cont)+'. Creative Commons License of the dataset: \\n')\n",
    "            f.write('\\t'+dataset.json()['data']['latestVersion']['license']['name']+'\\n\\n')\n",
    "        if 'persistentUrl' in dataset.json()['data']:\n",
    "            cont+=1\n",
    "            f.write(str(cont)+'. Dataset DOI: \\n')\n",
    "            f.write('\\t'+dataset.json()['data']['persistentUrl']+'\\n\\n')\n",
    "        if 'publicationCitation' in citation_keys or 'publicationIDType'in citation_keys or 'publicationIDNumber' in citation_keys or 'publicationURL' in citation_keys:\n",
    "            cont+=1\n",
    "            f.write(str(cont)+'. Related publication:\\n')\n",
    "            index=[]\n",
    "            keys=['publicationCitation','publicationIDType','publicationIDNumber','publicationURL']\n",
    "            specified_keys = [element for element in keys if element in citation_keys]\n",
    "            extracted_values = find_keys(citation_keys, specified_keys, citation_values)\n",
    "            for entry in extracted_values:\n",
    "                for key, value in entry.items():\n",
    "                    formatted_key = format_key(key)\n",
    "                    f.write('\\t'+f'{formatted_key}: {value}\\n')\n",
    "                f.write('\\n')\n",
    "        if len(geo_keys) != 0:\n",
    "            f.write('Geospatial Metadata \\n--------------------------------------\\n')\n",
    "            cont=0\n",
    "            if 'country' in geo_keys or 'state' in geo_keys or 'city' in geo_keys or 'otherGeographicCoverage' in geo_keys:\n",
    "                cont+=1\n",
    "                f.write(str(cont)+'. Geographical location/s of data collection:\\n')\n",
    "                keys=['country','state','city','otherGeographicCoverage']\n",
    "                specified_keys=[]\n",
    "                specified_keys = [element for element in keys if element in geo_keys]\n",
    "                extracted_values = find_keys(geo_keys, specified_keys, geo_values)\n",
    "                for entry in extracted_values:\n",
    "                    for key, value in entry.items():\n",
    "                        formatted_key = format_key(key)\n",
    "                        f.write('\\t'+f'{formatted_key}: {value}\\n')\n",
    "                    f.write('\\n')\n",
    "            if 'geographicUnit' in geo_keys:\n",
    "                cont+=1\n",
    "                f.write(str(cont)+'.  Geographic Unit:\\n')\n",
    "                auxiliar=[]\n",
    "                auxiliar.append(list_duplicates_of(geo_keys, 'geographicUnit'))\n",
    "                for i in auxiliar[0]:\n",
    "                    f.write('\\t')\n",
    "                    f.write(geo_values[i])\n",
    "                    if i != auxiliar[0][-1]:\n",
    "                        f.write('\\n ')\n",
    "                f.write('\\n\\n')\n",
    "            if 'westLongitude' in geo_keys or 'eastLongitude' in geo_keys or 'northLongitude' in geo_keys or 'southLongitude' in geo_keys:\n",
    "                keys=['westLongitude','eastLongitude','northLongitude','southLongitude']\n",
    "                specified_keys=[]\n",
    "                specified_keys = [element for element in keys if element in geo_keys]\n",
    "                extracted_values = find_keys(geo_keys, specified_keys, geo_values)\n",
    "                for entry in extracted_values:\n",
    "                    for key, value in entry.items():\n",
    "                        formatted_key = format_key(key)\n",
    "                        f.write('\\t'+f'{formatted_key}: {value}\\n')\n",
    "                    f.write('\\n')\n",
    "        if len(social_keys) != 0:\n",
    "            f.write('Social Science and Humanities Metadata \\n--------------------------------------\\n')\n",
    "            cont=0\n",
    "            if 'unitOfAnalysis' in social_keys:\n",
    "                cont+=1\n",
    "                f.write(str(cont)+'.  Unit of Analysis:\\n')\n",
    "                auxiliar=[]\n",
    "                auxiliar.append(list_duplicates_of(social_keys, 'unitOfAnalysis'))\n",
    "                for i in auxiliar[0]:\n",
    "                    f.write('\\t')\n",
    "                    f.write(social_values[i])\n",
    "                    if i != auxiliar[0][-1]:\n",
    "                        f.write('\\n ')\n",
    "                f.write('\\n\\n')\n",
    "            if 'universe' in social_keys:\n",
    "                cont+=1\n",
    "                f.write(str(cont)+'.  Universe:\\n')\n",
    "                auxiliar=[]\n",
    "                auxiliar.append(list_duplicates_of(social_keys, 'universe'))\n",
    "                for i in auxiliar[0]:\n",
    "                    f.write('\\t')\n",
    "                    f.write(social_values[i])\n",
    "                    if i != auxiliar[0][-1]:\n",
    "                        f.write('\\n ')\n",
    "                f.write('\\n\\n')\n",
    "            if 'timeMethod' in social_keys:\n",
    "                cont+=1\n",
    "                f.write(str(cont)+'.  Time method:\\n')\n",
    "                f.write('\\t'+social_values[social_keys.index('timeMethod')]+'\\n\\n')\n",
    "            if 'dataCollector' in social_keys:\n",
    "                cont+=1\n",
    "                f.write(str(cont)+'.  Data Collector:\\n')\n",
    "                f.write('\\t'+social_values[social_keys.index('dataCollector')]+'\\n\\n')\n",
    "            if 'collectorTraining' in social_keys:\n",
    "                cont+=1\n",
    "                f.write(str(cont)+'.  Collector Training:\\n')\n",
    "                f.write('\\t'+social_values[social_keys.index('collectorTraining')]+'\\n\\n')\n",
    "            if 'frequencyOfDataCollection' in social_keys:\n",
    "                cont+=1\n",
    "                f.write(str(cont)+'.  Frequency:\\n')\n",
    "                f.write('\\t'+social_values[social_keys.index('frequencyOfDataCollection')]+'\\n\\n')\n",
    "            if 'samplingProcedure' in social_keys:\n",
    "                cont+=1\n",
    "                f.write(str(cont)+'.  Sampling Procedure:\\n')\n",
    "                f.write('\\t'+social_values[social_keys.index('samplingProcedure')]+'\\n\\n')\n",
    "            if 'targetSampleActualSize' in social_keys or 'targetSampleSizeFormula' in social_keys:\n",
    "                cont+=1\n",
    "                f.write(str(cont)+'. Target Sample Size:\\n')\n",
    "                index=[]\n",
    "                keys=['targetSampleActualSize','targetSampleSizeFormula']\n",
    "                specified_keys=[]\n",
    "                specified_keys = [element for element in keys if element in social_keys]\n",
    "                extracted_values = find_keys(social_keys, specified_keys, social_values)\n",
    "                for entry in extracted_values:\n",
    "                    for key, value in entry.items():\n",
    "                        formatted_key = format_key(key)\n",
    "                        f.write('\\t'+f'{formatted_key}: {value}\\n')\n",
    "                    f.write('\\n')\n",
    "            if 'deviationsFromSampleDesign' in social_keys:\n",
    "                cont+=1\n",
    "                f.write(str(cont)+'.  Major Deviations for Sample Design:\\n')\n",
    "                f.write('\\t'+social_values[social_keys.index('deviationsFromSampleDesign')]+'\\n\\n')\n",
    "            if 'collectionMode' in social_keys:\n",
    "                cont+=1\n",
    "                f.write(str(cont)+'.  Collection Mode:\\n')\n",
    "                auxiliar=[]\n",
    "                auxiliar.append(list_duplicates_of(social_keys, 'collectionMode'))\n",
    "                for i in auxiliar[0]:\n",
    "                    f.write('\\t')\n",
    "                    f.write(social_values[i])\n",
    "                    if i != auxiliar[0][-1]:\n",
    "                        f.write('\\n ')\n",
    "                f.write('\\n\\n')\n",
    "            if 'researchInstrument' in social_keys:\n",
    "                cont+=1\n",
    "                f.write(str(cont)+'.  Type of Research Instrument:\\n')\n",
    "                f.write('\\t'+social_values[social_keys.index('researchInstrument')]+'\\n\\n')\n",
    "            if 'dataCollectionSituation' in social_keys:\n",
    "                cont+=1\n",
    "                f.write(str(cont)+'.  Characteristics of Data Collection Situation:\\n')\n",
    "                f.write('\\t'+social_values[social_keys.index('dataCollectionSituation')]+'\\n\\n')\n",
    "            if 'actionsToMinimizeLoss' in social_keys:\n",
    "                cont+=1\n",
    "                f.write(str(cont)+'.  Actions to Minimize Losses:\\n')\n",
    "                f.write('\\t'+social_values[social_keys.index('actionsToMinimizeLoss')]+'\\n\\n')\n",
    "            if 'controlOperations' in social_keys:\n",
    "                cont+=1\n",
    "                f.write(str(cont)+'.  Control Operations:\\n')\n",
    "                f.write('\\t'+social_values[social_keys.index('controlOperations')]+'\\n\\n')\n",
    "            if 'weighting' in social_keys:\n",
    "                cont+=1\n",
    "                f.write(str(cont)+'.  Weighting:\\n')\n",
    "                f.write('\\t'+social_values[social_keys.index('weighting')]+'\\n\\n')\n",
    "            if 'cleaningOperations' in social_keys:\n",
    "                cont+=1\n",
    "                f.write(str(cont)+'.  Cleaning Operations:\\n')\n",
    "                f.write('\\t'+social_values[social_keys.index('cleaningOperations')]+'\\n\\n')\n",
    "            if 'datasetLevelErrorNotes' in social_keys:\n",
    "                cont+=1\n",
    "                f.write(str(cont)+'.  Study Level Error Notes:\\n')\n",
    "                f.write('\\t'+social_values[social_keys.index('datasetLevelErrorNotes')]+'\\n\\n')\n",
    "            if 'responseRate' in social_keys:\n",
    "                cont+=1\n",
    "                f.write(str(cont)+'.  Response Rate:\\n')\n",
    "                f.write('\\t'+social_values[social_keys.index('responseRate')]+'\\n\\n')\n",
    "            if 'samplingErrorEstimates' in social_keys:\n",
    "                cont+=1\n",
    "                f.write(str(cont)+'.  Estimates of Sampling Error:\\n')\n",
    "                f.write('\\t'+social_values[social_keys.index('samplingErrorEstimates')]+'\\n\\n')\n",
    "            if 'otherDataAppraisal' in social_keys:\n",
    "                cont+=1\n",
    "                f.write(str(cont)+'.  Other Forms of Data Appraisal:\\n')\n",
    "                f.write('\\t'+social_values[social_keys.index('otherDataAppraisal')]+'\\n\\n')\n",
    "            if 'socialScienceNotesType' in social_keys or 'socialScienceNotesSubject' in social_keys or 'socialScienceNotesText' in social_keys :\n",
    "                cont+=1\n",
    "                f.write(str(cont)+'. Notes:\\n')\n",
    "                index=[]\n",
    "                keys=['socialScienceNotesType','socialScienceNotesSubject','socialScienceNotesText']\n",
    "                specified_keys=[]\n",
    "                specified_keys = [element for element in keys if element in social_keys]\n",
    "                extracted_values = find_keys(social_keys, specified_keys, social_values)\n",
    "                for entry in extracted_values:\n",
    "                    for key, value in entry.items():\n",
    "                        formatted_key = format_key(key)\n",
    "                        f.write('\\t'+f'{formatted_key}: {value}\\n')\n",
    "                    f.write('\\n')\n",
    "        if len(astronomy_keys) != 0:\n",
    "            f.write('Astronomy and Astrophysics Metadata  \\n--------------------------------------\\n')\n",
    "            cont=0\n",
    "            if 'astroType' in astronomy_keys:\n",
    "                cont+=1\n",
    "                f.write(str(cont)+'.  Type:\\n')\n",
    "                auxiliar=[]\n",
    "                auxiliar.append(list_duplicates_of(astronomy_keys, 'astroType'))\n",
    "                for i in auxiliar[0]:\n",
    "                    f.write('\\t')\n",
    "                    f.write(astronomy_values[i])\n",
    "                    if i != auxiliar[0][-1]:\n",
    "                        f.write('\\n ')\n",
    "                f.write('\\n\\n')\n",
    "            if 'astroFacility' in astronomy_keys:\n",
    "                cont+=1\n",
    "                f.write(str(cont)+'.  Facility:\\n')\n",
    "                auxiliar=[]\n",
    "                auxiliar.append(list_duplicates_of(astronomy_keys, 'astroFacility'))\n",
    "                for i in auxiliar[0]:\n",
    "                    f.write('\\t')\n",
    "                    f.write(astronomy_values[i])\n",
    "                    if i != auxiliar[0][-1]:\n",
    "                        f.write('\\n ')\n",
    "                f.write('\\n\\n')\n",
    "            if 'astroInstrument' in astronomy_keys:\n",
    "                cont+=1\n",
    "                f.write(str(cont)+'.  Instrument:\\n')\n",
    "                auxiliar=[]\n",
    "                auxiliar.append(list_duplicates_of(astronomy_keys, 'astroInstrument'))\n",
    "                for i in auxiliar[0]:\n",
    "                    f.write('\\t')\n",
    "                    f.write(astronomy_values[i])\n",
    "                    if i != auxiliar[0][-1]:\n",
    "                        f.write('\\n ')\n",
    "                f.write('\\n\\n')\n",
    "            if 'astroObject' in astronomy_keys:\n",
    "                cont+=1\n",
    "                f.write(str(cont)+'.  Object:\\n')\n",
    "                auxiliar=[]\n",
    "                auxiliar.append(list_duplicates_of(astronomy_keys, 'astroObject'))\n",
    "                for i in auxiliar[0]:\n",
    "                    f.write('\\t')\n",
    "                    f.write(astronomy_values[i])\n",
    "                    if i != auxiliar[0][-1]:\n",
    "                        f.write('\\n ')\n",
    "                f.write('\\n\\n')\n",
    "            if 'resolution.Spatial' in astronomy_keys:\n",
    "                cont+=1\n",
    "                f.write(str(cont)+'.  Spatial Resolution:\\n')\n",
    "                f.write('\\t'+astronomy_values[astronomy_keys.index('resolution.Spatial')]+'\\n\\n')\n",
    "            if 'resolution.Spectral' in astronomy_keys:\n",
    "                cont+=1\n",
    "                f.write(str(cont)+'.  Spectral Resolution:\\n')\n",
    "                f.write('\\t'+astronomy_values[astronomy_keys.index('resolution.Spectral')]+'\\n\\n')\n",
    "            if 'resolution.Temporal' in astronomy_keys:\n",
    "                cont+=1\n",
    "                f.write(str(cont)+'.  Temporal Resolution:\\n')\n",
    "                f.write('\\t'+astronomy_values[astronomy_keys.index('resolution.Temporal')]+'\\n\\n')\n",
    "            if 'coverage.Spectral.Bandpass' in astronomy_keys:\n",
    "                cont+=1\n",
    "                f.write(str(cont)+'.  Bandpass:\\n')\n",
    "                auxiliar=[]\n",
    "                auxiliar.append(list_duplicates_of(astronomy_keys, 'coverage.Spectral.Bandpass'))\n",
    "                for i in auxiliar[0]:\n",
    "                    f.write('\\t')\n",
    "                    f.write(astronomy_values[i])\n",
    "                    if i != auxiliar[0][-1]:\n",
    "                        f.write('\\n ')\n",
    "                f.write('\\n\\n')\n",
    "            if 'coverage.Spectral.CentralWavelength' in astronomy_keys:\n",
    "                cont+=1\n",
    "                f.write(str(cont)+'.  Central Wavelength (m):\\n')\n",
    "                auxiliar=[]\n",
    "                auxiliar.append(list_duplicates_of(astronomy_keys, 'coverage.Spectral.CentralWavelength'))\n",
    "                for i in auxiliar[0]:\n",
    "                    f.write('\\t')\n",
    "                    f.write(astronomy_values[i])\n",
    "                    if i != auxiliar[0][-1]:\n",
    "                        f.write('\\n ')\n",
    "                f.write('\\n\\n')\n",
    "            if 'coverage.Spectral.MinimumWavelength' in astronomy_keys or 'coverage.Spectral.MaximumWavelength' in astronomy_keys:\n",
    "                cont+=1\n",
    "                f.write(str(cont)+'. Wavelength Range:\\n')\n",
    "                index=[]\n",
    "                keys=['coverage.Spectral.MinimumWavelength','coverage.Spectral.MaximumWavelength']\n",
    "                specified_keys=[]\n",
    "                specified_keys = [element for element in keys if element in astronomy_keys]\n",
    "                extracted_values = find_keys(astronomy_keys, specified_keys, astronomy_values)\n",
    "                for entry in extracted_values:\n",
    "                    for key, value in entry.items():\n",
    "                        formatted_key = format_key(key)\n",
    "                        f.write('\\t'+f'{formatted_key}: {value}\\n')\n",
    "                    f.write('\\n')\n",
    "            if  'coverage.Temporal.StartTime' in astronomy_keys or  'coverage.Temporal.StartTime' in astronomy_keys:\n",
    "                cont+=1\n",
    "                f.write(str(cont)+'. Dataset Date Range:\\n')\n",
    "                index=[]\n",
    "                keys=['coverage.Temporal.StartTime', 'coverage.Temporal.StartTime']\n",
    "                specified_keys=[]\n",
    "                specified_keys = [element for element in keys if element in astronomy_keys]\n",
    "                extracted_values = find_keys(astronomy_keys, specified_keys, astronomy_values)\n",
    "                for entry in extracted_values:\n",
    "                    for key, value in entry.items():\n",
    "                        formatted_key = format_key(key)\n",
    "                        f.write('\\t'+f'{formatted_key}: {value}\\n')\n",
    "                    f.write('\\n')\n",
    "            if 'coverage.Spatial' in astronomy_keys:\n",
    "                cont+=1\n",
    "                f.write(str(cont)+'.  Sky Coverage:\\n')\n",
    "                auxiliar=[]\n",
    "                auxiliar.append(list_duplicates_of(astronomy_keys, 'coverage.Spatial'))\n",
    "                for i in auxiliar[0]:\n",
    "                    f.write('\\t')\n",
    "                    f.write(astronomy_values[i])\n",
    "                    if i != auxiliar[0][-1]:\n",
    "                        f.write('\\n ')\n",
    "                f.write('\\n\\n')\n",
    "            if 'coverage.Depth' in astronomy_keys:\n",
    "                cont+=1\n",
    "                f.write(str(cont)+'.  Depth Coverage:\\n')\n",
    "                f.write('\\t'+astronomy_values[astronomy_keys.index('coverage.Depth')]+'\\n\\n')\n",
    "            if 'coverage.ObjectDensity' in astronomy_keys:\n",
    "                cont+=1\n",
    "                f.write(str(cont)+'.  Object Density:\\n')\n",
    "                f.write('\\t'+astronomy_values[astronomy_keys.index('coverage.ObjectDensity')]+'\\n\\n')\n",
    "            if 'coverage.ObjectCount' in astronomy_keys:\n",
    "                cont+=1\n",
    "                f.write(str(cont)+'.  Object Count:\\n')\n",
    "                f.write('\\t'+astronomy_values[astronomy_keys.index('coverage.ObjectCount')]+'\\n\\n')\n",
    "            if 'coverage.SkyFraction' in astronomy_keys:\n",
    "                cont+=1\n",
    "                f.write(str(cont)+'.  Fraction of Sky :\\n')\n",
    "                f.write('\\t'+astronomy_values[astronomy_keys.index('coverage.SkyFraction')]+'\\n\\n')\n",
    "            if 'coverage.Polarization' in astronomy_keys:\n",
    "                cont+=1\n",
    "                f.write(str(cont)+'.  Polarization:\\n')\n",
    "                f.write('\\t'+astronomy_values[astronomy_keys.index('coverage.Polarization')]+'\\n\\n')\n",
    "            if 'redshiftType' in astronomy_keys:\n",
    "                cont+=1\n",
    "                f.write(str(cont)+'.  RedshiftType:\\n')\n",
    "                f.write('\\t'+astronomy_values[astronomy_keys.index('redshiftType')]+'\\n\\n')\n",
    "            if 'resolution.Redshift' in astronomy_keys:\n",
    "                cont+=1\n",
    "                f.write(str(cont)+'.  Redshift Resolution:\\n')\n",
    "                f.write('\\t'+astronomy_values[astronomy_keys.index('resolution.Redshift')]+'\\n\\n')\n",
    "            if  'coverage.Redshift.MinimumValue' in astronomy_keys or  'coverage.Redshift.MaximumValue' in astronomy_keys:\n",
    "                cont+=1\n",
    "                f.write(str(cont)+'. Redshift Value:\\n')\n",
    "                index=[]\n",
    "                keys=['coverage.Redshift.MinimumValue','coverage.Redshift.MaximumValue']\n",
    "                specified_keys=[]\n",
    "                specified_keys = [element for element in keys if element in astronomy_keys]\n",
    "                extracted_values = find_keys(astronomy_keys, specified_keys, astronomy_values)\n",
    "                for entry in extracted_values:\n",
    "                    for key, value in entry.items():\n",
    "                        formatted_key = format_key(key)\n",
    "                        f.write('\\t'+f'{formatted_key}: {value}\\n')\n",
    "                    f.write('\\n')\n",
    "        if len(biomedical_keys) != 0:\n",
    "            f.write('Life Sciences Metadata \\n-------------------------\\n')\n",
    "            cont=0\n",
    "            if 'studyDesignType' in biomedical_keys:\n",
    "                cont+=1\n",
    "                f.write(str(cont)+'.  Design Type:\\n')\n",
    "                auxiliar=[]\n",
    "                auxiliar.append(list_duplicates_of(biomedical_keys, 'studyDesignType'))\n",
    "                for i in auxiliar[0]:\n",
    "                    f.write('\\t')\n",
    "                    f.write(biomedical_values[i])\n",
    "                    if i != auxiliar[0][-1]:\n",
    "                        f.write('\\n ')\n",
    "                f.write('\\n\\n')\n",
    "            if 'studyFactorType' in biomedical_keys:\n",
    "                cont+=1\n",
    "                f.write(str(cont)+'.  Factor Type:\\n')\n",
    "                auxiliar=[]\n",
    "                auxiliar.append(list_duplicates_of(biomedical_keys, 'studyFactorType'))\n",
    "                for i in auxiliar[0]:\n",
    "                    f.write('\\t')\n",
    "                    f.write(biomedical_values[i])\n",
    "                    if i != auxiliar[0][-1]:\n",
    "                        f.write('\\n ')\n",
    "                f.write('\\n\\n')\n",
    "            if 'studyAssayOrganism' in biomedical_keys:\n",
    "                cont+=1\n",
    "                f.write(str(cont)+'.  Organism:\\n')\n",
    "                auxiliar=[]\n",
    "                auxiliar.append(list_duplicates_of(biomedical_keys, 'studyAssayOrganism'))\n",
    "                for i in auxiliar[0]:\n",
    "                    f.write('\\t')\n",
    "                    f.write(biomedical_values[i])\n",
    "                    if i != auxiliar[0][-1]:\n",
    "                        f.write('\\n ')\n",
    "                f.write('\\n\\n')\n",
    "            if 'studyAssayOtherOrganism' in biomedical_keys:\n",
    "                cont+=1\n",
    "                f.write(str(cont)+'.  Other Organism:\\n')\n",
    "                auxiliar=[]\n",
    "                auxiliar.append(list_duplicates_of(biomedical_keys, 'studyAssayOtherOrganism'))\n",
    "                for i in auxiliar[0]:\n",
    "                    f.write('\\t')\n",
    "                    f.write(biomedical_values[i])\n",
    "                    if i != auxiliar[0][-1]:\n",
    "                        f.write('\\n ')\n",
    "                f.write('\\n\\n')\n",
    "            if 'studyAssayMeasurementType' in biomedical_keys:\n",
    "                cont+=1\n",
    "                f.write(str(cont)+'.  Measurement Type:\\n')\n",
    "                auxiliar=[]\n",
    "                auxiliar.append(list_duplicates_of(biomedical_keys, 'studyAssayMeasurementType'))\n",
    "                for i in auxiliar[0]:\n",
    "                    f.write('\\t')\n",
    "                    f.write(biomedical_values[i])\n",
    "                    if i != auxiliar[0][-1]:\n",
    "                        f.write('\\n ')\n",
    "                f.write('\\n\\n')\n",
    "            if 'studyAssayOtherMeasurmentType' in biomedical_keys:\n",
    "                cont+=1\n",
    "                f.write(str(cont)+'.  Other Measurement Type:\\n')\n",
    "                auxiliar=[]\n",
    "                auxiliar.append(list_duplicates_of(biomedical_keys, 'studyAssayOtherMeasurmentType'))\n",
    "                for i in auxiliar[0]:\n",
    "                    f.write('\\t')\n",
    "                    f.write(biomedical_values[i])\n",
    "                    if i != auxiliar[0][-1]:\n",
    "                        f.write('\\n ')\n",
    "                f.write('\\n\\n')\n",
    "            if 'studyAssayTechnologyType' in biomedical_keys:\n",
    "                cont+=1\n",
    "                f.write(str(cont)+'.  Technology Type:\\n')\n",
    "                auxiliar=[]\n",
    "                auxiliar.append(list_duplicates_of(biomedical_keys, 'studyAssayTechnologyType'))\n",
    "                for i in auxiliar[0]:\n",
    "                    f.write('\\t')\n",
    "                    f.write(biomedical_values[i])\n",
    "                    if i != auxiliar[0][-1]:\n",
    "                        f.write('\\n ')\n",
    "                f.write('\\n\\n')\n",
    "            if 'studyAssayPlatform' in biomedical_keys:\n",
    "                cont+=1\n",
    "                f.write(str(cont)+'.  Technology Platform:\\n')\n",
    "                auxiliar=[]\n",
    "                auxiliar.append(list_duplicates_of(biomedical_keys, 'studyAssayPlatform'))\n",
    "                for i in auxiliar[0]:\n",
    "                    f.write('\\t')\n",
    "                    f.write(biomedical_values[i])\n",
    "                    if i != auxiliar[0][-1]:\n",
    "                        f.write('\\n ')\n",
    "                f.write('\\n\\n')\n",
    "            if 'studyAssayCellType' in biomedical_keys:\n",
    "                cont+=1\n",
    "                f.write(str(cont)+'.  Cell Type:\\n')\n",
    "                auxiliar=[]\n",
    "                auxiliar.append(list_duplicates_of(biomedical_keys, 'studyAssayCellType'))\n",
    "                for i in auxiliar[0]:\n",
    "                    f.write('\\t')\n",
    "                    f.write(biomedical_values[i])\n",
    "                    if i != auxiliar[0][-1]:\n",
    "                        f.write('\\n ')\n",
    "                f.write('\\n\\n')\n",
    "        if len(journal_keys) != 0:\n",
    "            f.write('Journal Metadata \\n-------------------------\\n')\n",
    "            cont=0\n",
    "            if 'journalVolume' in journal_keys or 'journalIssue' in journal_keys or 'journalPubDate' in journal_keys:\n",
    "                cont+=1\n",
    "                f.write(str(cont)+'. Journal:\\n')\n",
    "                index=[]\n",
    "                keys=['journalVolume', 'journalIssue','journalPubDate']\n",
    "                specified_keys=[]\n",
    "                specified_keys = [element for element in keys if element in journal_keys]\n",
    "                extracted_values = find_keys(journal_keys, specified_keys, journal_values)\n",
    "                for entry in extracted_values:\n",
    "                    for key, value in entry.items():\n",
    "                        formatted_key = format_key(key)\n",
    "                        f.write('\\t'+f'{formatted_key}: {value}\\n')\n",
    "                    f.write('\\n')\n",
    "            if 'journalArticleType' in journal_keys:\n",
    "                cont+=1\n",
    "                f.write(str(cont)+'.  Type of Article:\\n')\n",
    "                f.write('\\t'+journal_values[journal_keys.index('journalArticleType')]+'\\n\\n')\n",
    "        f.write('FILE OVERVIEW\\n----------------------\\n')\n",
    "        for i in range(0,len(filemetadata_keys)):\n",
    "            f.write('\\t'+'File name: '+filemetadata_values[i][filemetadata_keys[i].index('filename')]+'\\n')\n",
    "            if 'description' in filemetadata_keys[i]:\n",
    "                f.write('\\t'+'Decription: '+filemetadata_values[i][filemetadata_keys[i].index('description')]+'\\n')\n",
    "            f.write('\\t'+'File format: '+filemetadata_values[i][filemetadata_keys[i].index('contentType')]+'\\n\\n')\n",
    "        print('The Readme has been created in the directory' + path +'.')\n",
    "\n",
    "# Checking if both inputs are provided\n",
    "if not doi or not token:\n",
    "    print(\"Please enter DOI, Token and URL of the repository correctly.\")\n",
    "else:\n",
    "    api = NativeApi(base_url, token)\n",
    "    data_api = DataAccessApi(base_url, token)\n",
    "    dataset = api.get_dataset(doi)\n",
    "\n",
    "    #  Metadata lists:\n",
    "    citation_keys, geo_keys, social_keys, astronomy_keys, biomedical_keys, journal_keys = [[] for _ in range(6)]\n",
    "    citation_values, geo_values, social_values, astronomy_values, biomedical_values, journal_values = [[] for _ in range(6)]\n",
    "    filemetadata_keys=[]\n",
    "    filemetadata_values=[]\n",
    "\n",
    "    # Exporting metadata and creating readme\n",
    "    exportmetadata(base_url, token, doi, citation_keys, citation_values, geo_keys, geo_values, social_keys,\n",
    "                   social_values, astronomy_keys, astronomy_values, biomedical_keys, biomedical_values,\n",
    "                   journal_keys, journal_values)\n",
    "    filemetadata(base_url, token, doi, filemetadata_keys, filemetadata_values)\n",
    "    createreadme(base_url, token, doi, citation_keys, citation_values, geo_keys, geo_values, social_keys,\n",
    "                 social_values, astronomy_keys, astronomy_values, biomedical_keys, biomedical_values,\n",
    "                 journal_keys, journal_values, filemetadata_keys, filemetadata_values)\n",
    "\n",
    "    # Construct the correct file path\n",
    "file_path = os.path.join(f'{doi.replace(\"doi:10.34810/\", \"\")}', 'Readme.txt')\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Running in Google Colab\n",
    "    # Provide a download button\n",
    "    download_button = widgets.Button(description=\"Download Readme\")\n",
    "    display(download_button)\n",
    "\n",
    "    # Function to be executed when the download button is clicked\n",
    "    def on_download_button_click(b):\n",
    "        # Download the Readme.txt file in Google Colab\n",
    "        files.download(file_path)\n",
    "\n",
    "    # Event handler for the download button\n",
    "    download_button.on_click(on_download_button_click)\n",
    "\n",
    "else:\n",
    "    # Running in Jupyter Notebook\n",
    "    # Provide a download link\n",
    "    download_link = FileLink(file_path, result_html_prefix=\"Click to view the Readme: \")\n",
    "    display(download_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "2fa080e4e81e44479d2e76ca5327f6b7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "button_color": null,
      "font_weight": ""
     }
    },
    "549357a5ba174f0baaa44897b60783f3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ButtonView",
      "button_style": "",
      "description": "Instal·lar llibreries",
      "disabled": false,
      "icon": "",
      "layout": "IPY_MODEL_acb2ab1485364bb9bbc12e0ff226b28c",
      "style": "IPY_MODEL_2fa080e4e81e44479d2e76ca5327f6b7",
      "tooltip": ""
     }
    },
    "6e228675cbb042cbb2d3604308c8d5c6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "button_color": null,
      "font_weight": ""
     }
    },
    "93520107254c45a9b49e4c1fc05da9ec": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ButtonView",
      "button_style": "",
      "description": "Descarregar Readme",
      "disabled": false,
      "icon": "",
      "layout": "IPY_MODEL_bbe678f714d148ffa454f85278baaa1e",
      "style": "IPY_MODEL_6e228675cbb042cbb2d3604308c8d5c6",
      "tooltip": ""
     }
    },
    "acb2ab1485364bb9bbc12e0ff226b28c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bbe678f714d148ffa454f85278baaa1e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
